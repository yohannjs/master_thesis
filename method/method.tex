\chapter{Method}

\section{Description of The Datasets} \label{sec:datasets}

Since the different \acrshort{ml} models require different types of input data the, datasets have been divided into two main categories: 
The peak-value datasets and the time-series datasets. \bigskip

\subsection{Time-series Datasets} \label{sec:ts_dsets}

\begin{table*}[h]
    \centering
    \ra{1.3}
    \begin{tabular}{ rlr }
        \toprule
        Nr & Input variables   & Shape \\
        \midrule
        1  & Single \acrshort{rls} curves & $(3600,1)$ \\
        2  & \acrshort{rls} curves        & $(200,18)$ \\
        3  & \acrshort{gls} curves        & $(200,3)$  \\
        4  & Strain curves                & $(200,21)$ \\
        \bottomrule
    \end{tabular}
    \caption{Time-series datasets. The ''Shape'' parameter is indicates: (Number of objects in the dataset, Number of curves used to represent each individual object). The curve length is not included in the shape parameter because it differs for different curves.}
    \label{tab:ts_dsets}
\end{table*}

Table \ref{tab:ts_dsets} shows the different time-series datasets that will be used. 
All the datasets except \textit{Single \acrshort{rls} curves} will be used to predict whether or not the patient is diagnosed, and whether the patient has heart failure. Recall that the different diagnosises are described in section REFERENCE, and there occurance rate are illustrated in figure \ref{fig:hf_ind_dist}.\textit{Single \acrshort{rls} curves} will be used to predict the segment indications shown in figure \ref{fig:segm_label_dist} and described in section REFERENCE. The point of classifying individual segments of a patients left ventricle is that if a single segment is found to be \textit{not normal}, this would also mean that the patient can be considered as \textit{not healthy}. As mentioned in the description of table \ref{tab:ts_dsets} the ''Shape'' parameter shows how many objects each dataset has, and how many curves are associated to each object. Since each ultrasound examination takes ultrasound inspections from three views (four chamber, two chamber, and APLAX chamber), each patient has three views to estimate a \acrshort{gls} curve from. Since each \acrshort{gls} curve, also can be divided into six \acrshort{rls} curves, there is a total of 21 strain curves per patient. Since each patient has 18 \acrshort{rls} curves, there are $18 \times 200 = 3600$ curves that make up dataset number 1. For datasets two to three it will also be experimented with wether using data from a single view performs better than data from all views. For dataset two that means that the number of curves used to represent an object will be either 6 or 18, for dataset three it will be either 1 or 3 curves and for dataset four patients will be represented with either 7 or 21 curves. Both the \acrshort{ann}, and the \acrshort{tsc} model are applied on the datasets listed in table \ref{tab:ts_dsets}. \bigskip

\subsection{Peak-value Datasets}

\begin{table*}[h]
    \centering
    \ra{1.3}
    \begin{tabular}{ rlr }
        \toprule
        Nr & Input variables                                           & Shape \\
        \midrule                              
        1  & Peak systolic \acrshort{rls} values                       & $(200,18)$ \\
        2  & Peak systolic \acrshort{gls} values                       & $(200,3)$  \\
        3  & Peak systolic strain values                               & $(200,21)$ \\
        4  & Peak systolic \acrshort{rls}, and \acrshort{ef} values    & $(200,19)$ \\
        5  & Peak systolic \acrshort{gls}, and \acrshort{ef} values    & $(200,4)$  \\
        6  & Peak systolic strain, and \acrshort{ef} values            & $(200,22)$ \\
        \bottomrule
    \end{tabular}
    \caption{Peak-value datasets. The ''Shape'' parameter is indicates: (Number of objects in the dataset, Number of dimensions used to represent each individual object).}
    \label{tab:pv_dsets}
\end{table*}

Table \ref{tab:pv_dsets} shows the different peak-value datasets. All the datasets will be used to predict the diagnosis of patients, and whether the patient has heart failure. Single peak systolic \acrshort{rls} values were not considered suited for \acrshort{pvc} or \acrshort{pvsc} models to predict segment indication, because the best model one can hope for from a one dimensional point-value dataset is a form of threshold classifier. The reason that there are more peak-value datasets than there are time-series datasets, is that the peak-value version of three datasets in table \ref{tab:ts_dsets} have been combined with \acrshort{ef} to determine whether a combination of peak systolic strain, and \acrshort{ef} can have a higher predictive power than strain alone.

\section{Clustering} \label{sec:meth_clust}

The implementations of the two clustering models that are applied in this work are described together in the same section because conceptually, they are almost identical. It is only the method used to measure dissimilarity that separates the \acrshort{pvc} and \acrshort{tsc} models. The general implementation of the clustering models is illustrated in figure \ref{fig:clust_flow}. Time-series datasets are preprocessed before dissimilarity measurement, peak-value datasets are not. In the coming subsections the processes in each of the boxes in the flow diagram will be expanded upon.

\begin{figure}
    \centering
    \input{method/clust_flow}
    \caption{A flow diagram to give an overview of how the \acrshort{pvc} and \acrshort{tsc} models are implemented and evaluated.}
    \label{fig:clust_flow}
\end{figure}

\clearpage

\subsection{Time-series Preprocessing}
Preprocessing of the time series is done because it is known that the \acrshort{dtw} distance is sensitive to absolute difference, and offsets of time series. In addition to clustering the longitudinal strain time series without preprocessing three forms of preprocessing were tested to see whether they could improve the predictive performance of the clustering algorithm: Normalization, scaling and Z-score normalization. The normalized version of a time series ($\{x_t\}_N$) is calculated by equation \eqref{eq:ts_norm}. The smallest recorded value in the time series ($\mathrm{min}\{x_t\}$) is subtracted from the time series ($\{x_t\}$), then the time series is divided by the difference between the highest recorded value ($\mathrm{max}\{x_t\}$), and lowest recorded value in the time series.

\begin{equation}
    \{x_t\}_N = \frac{\{x_t\} - \mathrm{min}\{x_t\}}{\mathrm{max}\{x_t\} - \mathrm{min}\{x_t\}}
    \label{eq:ts_norm}
\end{equation}

The ''scaled'' version of a times is calculated in a similar fashion. Scaling can be considered as normalizing a time series with regard to the highest, and lowest recorded values of the entire set of time series it is being compared to. If one lets $\{ \left \{ x_t \} \right \}$ represent the set of time series to be scaled, $\mathrm{min}\{ \left \{ x_t \} \right \}$ represent the smallest recorded value in the entire set of time series and $\mathrm{max}\{ \left \{ x_t \} \right \}$ represent the highest recorded value in the set of time series, the scaled version of a time series ($\{x_t\}_S$) is given by eqation \eqref{eq:ts_scale}.

\begin{equation}
    \{x_t\}_S = \frac{\{x_t\} - \mathrm{min}\{ \left \{ x_t \} \right \}}{\mathrm{max}\{ \left \{ x_t \} \right \} - \mathrm{min}\{ \left \{ x_t \} \right \}}
    \label{eq:ts_scale}
\end{equation}

The Z-score normalization is done by transforming each observation of a time series to it's Z-score. The Z-score of an individual time-series observation is calculated by subracting the expected value of the time series, and dividing by the standard deviation. The unbiased estimators used to calculate the expected value, and standard deviation of a time series are given in equations \eqref{eq:ev_est}, and \eqref{eq:std_est} respectively. The Z-score normalized version of a time series ($\{x_t\}_Z$) is calculated using equation \eqref{eq:ts_zscn}

\begin{equation}
    \hat{\mu} = \frac{1}{n} \sum^n_{t = 1} x_t
    \label{eq:ev_est}
\end{equation}

\begin{equation}
    \hat{\sigma} = \sqrt{\frac{1}{n - 1} \sum^n_{t = 1} (x_t - \hat{\mu})^2}
    \label{eq:std_est}
\end{equation}

\begin{equation}
    \{x_t\}_Z = \frac{\{x_t\} - \hat{\mu}}{\hat{\sigma}}
    \label{eq:ts_zscn}
\end{equation}

Figure \ref{fig:preproc} illustrates how the different preprocessing methods work on the \acrshort{4ch} \acrshort{gls} curves of four random patients. By comparing \ref{fig:preproc}a and \ref{fig:preproc}d one can see that scaling preserves both the relative offsets and relative size differences between the curves. From \ref{fig:preproc}b one can see that though normalization preserves the offsets of the curves, the relative sizes are not. From \ref{fig:preproc}c one can see that Z-score normalization preserves the offsets of the curves, the relative sizes are only preserved to a certain extent. In addition, the normalized, and scaled curves are constricted between 0 and 1, while the Z-score normalized curves are not.  

\begin{figure}
    \centering
    \input{method/preproc_curves.pgf}
    \caption{Four plots of three random \acrshort{4ch} \acrshort{gls} curves that are preprocessed in the three different ways. (a) no preprocessing, (b) normalization, (c) Z-score normalization and (d) scaling}
    \label{fig:preproc}
\end{figure}

\subsection{Dissimilarity measurement}

When estimating dissimilarity between patients represented by a peak-value dataset Euclidean distance was used. To measure the dissimilarity between longitudinal strain curves in the \acrshort{tsc} model \acrshort{dtw} distanse was used. Recall that the \acrshort{dtw} distance between to time series is the length of the shortest \acrshort{dtw} path between them. To calculate the \acrshort{dtw} distance the \textbf{dtaidistance 1.2.5} library was used. The \textbf{dtaidistance} library is used by the DTAI Research Group to measure distances between time series. To encapsulate all the dissimilarity between patients in a single matrix, one first has to calculate one matrix of \acrshort{dtw} distances for each of the time series used to represent patients. Say that a patient was represented using the \acrshort{gls} curves in the three views. To calculate the dissimilarity matrix one would first estimate the \acrshort{dtw} distance between all the \acrshort{4ch} \acrshort{gls} curves, then all the \acrshort{2ch} \acrshort{gls} curves and finally all the \acrshort{aplax} \acrshort{gls} curves. By adding the three matrices of \acrshort{dtw} distances together one gets the dissimilarity matrix. \bigskip

\subsection{Hierarchical Agglomorative Clustering}

As mentioned in section REFERENCE, the hierarchical agglomorative clustering algorithm takes inn the dissimilarity matrix, and starts with every patient being represented by one cluster each. For each number of possible clusters then two clusters are merged based on minimizing one of the six linkage criterions. There are also various options of distance metrics that can be used by the clustering algorithm to measure difference between elements of the dissimilarity matrix, but in this work only Euclidean distance is used. The clustering algorithm used for time-series data is implemented using the \textbf{scipy.cluster.hierarchy 1.4.1} library, and in the \acrshort{tsc} model all six linkages detailed in section REFERENCE are tested. In the \acrshort{pvc} models a more holistic implementation is applied using the \textbf{scikit-learn 0.22.1} library. The implementation of the \acrshort{pvc} models does both the dissimilarity and clustering using one library, and because the scikit-learn library supports fewer linkage criteria only the single, complete, average and ward linkages are tested. 

\subsection{Cluster Assignment Evaluation}

When evaluating a specific \acrshort{tsc}, or \acrshort{pvc} clustering model, the model is evaluated at two to nine cluster cluster centers. For the cluster assignments given by evaluating the model at two cluster centers the models \acrshort{tp}, \acrshort{tn}, \acrshort{fp} and \acrshort{fn} are calculated. These metrics are then used to estimate the models accuracy, sensitivity, specificity and DOR. The cluster assignments for a clustering model evaluated at two cluster centers can be either 1 or 2, and since clustering is a form of unsupervised machine learning it is not given whether cluster 1 or 2 corresponds to the 1 or 0 of the target variable. Therefore, the evaluation metrics are calculated twice for each clustering model evaluated at two cluster centers, once where cluster 1 corresponds to target variable 1, and once where cluster 2 corresponds to target variable 1. The calculation that yields the highest accuracy is kept, and the other is disregarded. In addition to these matrics the ARI is used to evaluate all the cluster assignments yielded from evaluating a clustering model at between two to nine cluster centers. The ARI is used because it can give a measure of how correlated the distributions of the cluster centers are with regard to the distribution of the target variables. This can give insight into whether a clustering model evaluated at a higher number of cluster centers than two is better at capturing a particular target variable. In the heart failure, and patient diagnosis case studies there are twelve different datasets, four types of preprocessing, and seven different linkages tested. This yields a total of 336 variations of the \acrshort{tsc} model that are tested in the heart failure, and patient diagnosis case studies. In the segment indication case study, there are only 28 variations of the \acrshort{tsc} model tested since there is only one dataset. For the \acrshort{pvc} models there are six datasets tested, and four linkages tested yielding 24 variations of the \acrshort{pvc} model tested in the heart failure and patient diagnosis case studies.

\section{Artificial Neural Network} \label{sec:meth_nn}

\subsection{Preprocessing}

Two methods of preprocessing were tested on the data used as input for the \acrshort{ann} in addition to testing the \acrshort{ann} models witout preprocessing. Since neural networks with recurrent layers perform better when the sample rates of the input time series are equal, as they usually aren't correlated with the target variable. Since the frame rate of the ultrasound videos vary from patient to patient, the frame rates of the longitudinal strain curve time series also vary, and in this case sample rate is not correlated with heart failure, patient diagnosis or segment indication. To counteract with this it was tested whether upsampling all the strain curves to the highest sample rate, or downsampling them all to the lowest frame rate would affect the performance of the \acrshort{ann}. 

\subsection{Architecture}

The architecture of the \acrshort{ann} used in this work was not designed by the author himself, as there sadly was not enough time. The architecture was designed by student Benjamin Nedregård and was used to estimate heart phase, and patient health using blood flow curves as input. The reason why this architecture was applied is because it showed great promise when applied to blood flow time series, which share charactaristics with left ventricle longitudinal strain time series. The network was implemented using the \textbf{keras} with \textbf{tensorflow 2.1.0} as backend. The architecture used for the \acrshort{ann} is illustrated in figure \ref{fig:nn_arch}. One aspect that is not shown in figure \ref{fig:nn_arch} is the total numbe of trainable parameters of the architecture. The reason for this is because it varies based on the shape of the dataset it is applied on. In section \ref{sec:ts_dsets} the different time-series datasets that are used in this thesis are detailed. Recall that the different datasets will use different combinations of \acrshort{gls} and \acrshort{rls} curves from one or all of the three ultrasound views. Because of this the number of curves used as input for the \acrshort{ann} can be 1, 3, 6, 7, 18 or 21.

\begin{figure}
    \centering
    \input{method/nn_arch}
    \caption{A block diagram illustrating the architecture of the \acrshort{ann} used in this work.}
    \label{fig:nn_arch}
\end{figure}

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{ llrr }
        \toprule
        Strain curves used                              & Views used  & Nr. of time series & Nr. of trainable parameters \\
        \midrule                                     
        \acrshort{gls}, or single \acrshort{rls} curves & Single view &         1          & 39,457 \\
        \acrshort{gls} curves                           &  All views  &         3          & 43,553 \\
        \acrshort{rls} curves                           & Single view &         6          & 49,697 \\
        \acrshort{rls} curves                           &  All views  &        18          & 74,273 \\
        \acrshort{gls} and \acrshort{rls} curves        & Single view &         7          & 51,745 \\
        \acrshort{gls} and \acrshort{rls} curves        &  All views  &        21          & 80,417 \\
        \bottomrule
    \end{tabular}
    \caption{This table shows the total number of trainable parameters of the \acrshort{ann}, for different number of time-series inputs.}
    \label{tab:train_params}
\end{table*}

Since the author did not design the architecture of the network, a thorough defence of the architecture will not be given. However, a brief explanation of the properties the different layers contribute with to the model as a whole will be given. The two first layers in the \acrshort{ann} are convolutional, and are intended to detect simple structures in the time series such as linear regions, curved regions and rapid changes in the signal. The recurrent layer is intended to detect time dependant relations of the signal such as periodicity and frequency. Regularization terms are added to the outputs of the convolutional, and recurrent layers to attempt to bias the weights toward zero, which is a technique used to avoid overfitting.Finally the dense layers are intended to connect the features extracted by the previous layers to specific values the target variable can have, and make a prediction.

\subsection{Training and Validation}

Binary cross entropy was used as loss function during training of the variations of the \acrshort{ann} model. Each variation was trained for five epochs, using back propagation and \acrshort{sgd}. The ADAM learning rate optimizer was used with an initial learning rate of $10^{-3}$ with the intention of avoiding that the loss function of the \acrshort{ann} got stuck in local minima during training. The baises of each layer were initialized as zeros, and the weights of the individual units were initialized by sampling from the standard normal distribution function. \bigskip. 

To validate the \acrshort{ann} models, 10-fold cross-validation was used. $N$-fold cross validation of a model-dataset combination entails dividing the dataset into N chunks of equal size, and preferably with an approximately equal distribution of target-variable values in each chunk. Then in N rounds, called \textit{folds} $N-1$ chunks are used to train the model and the final chunk is used to test the model. For each round one also changes which chunk is used to test the model such that it is able to attempt making a prediction on every value of the dataset. When validating the variations of the \acrshort{ann} using cross-validation the number of \acrshort{tp}, \acrshort{tn}, \acrshort{fp} and \acrshort{fn} attained during each fold were recorded and added together after all the folds were complete. The sum of all \acrshort{tp}, \acrshort{tn}, \acrshort{fp} and \acrshort{fn} attained during cross-validation are used to estimate the models' accuracy, sensitivity, specificity and \acrshort{dor}. Since there are a total of twelve datasets and three types of preprocessing tested, there are a total of 36 variations of the \acrshort{ann} model applied in the heart failure and patient diagnosis case studies. In the segment indication case study there is only one dataset, and three forms of preprocessing tested, so there are only three variations of the \acrshort{ann} model tested.

\section{Peak-value Supervised Classifiers} \label{sec:meth_pvsc}

Eleven base supervised classifiers are included in the \acrshort{pvsc} model group, they are all implemented using the \textbf{scikit-learn 0.23.1} library. In this section a short description of the theory behind these classifiers, and the hyperparameters used in this work will be given. The \acrshort{pvsc} models are validated using 10-fold cross-validation in the same manner as the \acrshort{ann} models.

\subsection{Multi-layer Perceptron}
The \acrshort{mlp} is mentioned earlier described earlier in section \ref{sec:mlp}. This \acrshort{mlp} is configured with a single dense layer with a 100 neurons with the \acrshort{relu} activation, and an output layer of a single neuron since the classification problem is binary. It is trained with using \acrshort{sgd} with back-propagation, with ADAM as gradient descent optimizer, and an initial learning rate of $10^{-3}$.

\subsection{K Nearest Neighbors}
\acrfull{knn} is a machine learning model that can be used for classification and for regression. \acrshort{knn} are described as a form of \textit{lazy learner} because it does not extract generalized rules from the training set that are used to relate the input, and target variables, but instead memorizes the dataset \cite{python_machine_learning_2nd}. When used for classification the target variable is predicted based on the objects from the training set which are ''nearest'' in terms of input variable values. Hence, there are two central features that define a \acrshort{knn} classifier: The number of neighbors used for comparison, and the distance metric used to measure proximity to its neighbors \cite{python_machine_learning_2019}. In the implementation used in this work, the model was constricted to only consider five closest neighbors weighted equally, and use Euclidan distance as a distance metric. The implementation uses a combination of three algorithms to compute the nearest neighbors: BallTree, KDTree and brute force search. The BallTree and KDTree algorithms are constricted to a maximum of 30 leaves.

\subsection{Support Vector Classifier}
Support vector machines were originally implemented as a type of binary classifier that could classify linearly separable variables, as classifiers they are referred to as \acrfull{svc}. Under ideal conditions \acrshort{svc} transform input variables to a set of hyperplanes where the target variable values are linearly separable \cite{svm_wikipedia}. The transformation used depends on what kernel is used, some examples of kernel functions include: Linear kernal, \acrfull{rbf} and sigmoid function. In this work two versions of the \acrshort{svc} are tested, one with a linear kernel and one where the \acrshort{rbf} is used. The \acrshort{rbf} is given in in equation \eqref{eq:rbf}, where $\gamma$ is equal to 2. Both \acrshort{svc} applied use an $L_2$ regularization penalty to avoid overfitting. For the \acrshort{rbf} \acrshort{svc} parameter $C$ which is the inverse strength of the regularization is set to 1. For the linear \acrshort{svc} C is set to $0.025$.

\begin{equation}
    \mathrm{RBF}(\mathbf{x_1}, \mathbf{x_2}) = e^{\gamma \left ( \mathbf{x_1} - \mathbf{x_2} \right )^2}
    \label{eq:rbf}
\end{equation}

\subsection{Gaussian Process Classifier}
\acrfull{gp} are a probabilistic machine learning technique that can be used for regression, and classification tasks. Similar to \acrshort{svc} they perform best when the relationship between the input variables and the target variable are linear, but by the use of what is called \textit{basis functions} they can map the input variables to a hyperplane where the targets are linearly seperable \cite{GP_book}. One can say that basis functions are for \acrshort{gp}, what kernels are for \acrshort{svc}. The defining difference are that \acrshort{gp} are probabilistic while \acrshort{svc} are deterministic. Where \acrshort{svc} work with a single kernel \acrshort{gp} work with an infinite set of basis functions, and much of the training process amounts to finding an optimal linear combination of the set of kernals available \cite{GP_book}. The implementation of the \acrshort{gp} classifier in this thesis uses an \acrshort{rbf} function as covariance function with $\gamma$ equal to 0.5. The implementation uses the ''L-BFGS-B'' algorithm to optimize the basis functions parameters during training, and sets a maximum of 100 iterations of Newtons method during predict operations. 

\subsection{Naive Bayes Classifier}
The naive bayesian classifier used in this work is specifically a gaussian naive bayesian classifer, it is a probabilistic classifier based on Bayes rule, and the assumption that individual input features are independent. If one lets $\mathbf{X}$ be the training input data, $\mathbf{t}$ be the training target data, $x_{new}$ be a piece of new input data and $t_{new}$ be the corresponding new target. Bayes rule in this context is then given by equation \eqref{eq:bayes}.

\begin{equation}
    \mathrm{P} \left(t_{new}=k|\mathbf{X},\mathbf{t},x_{new}\right)=\frac{\mathrm{P}\left(x_{new}|t_{new}=k,\mathbf{X},\mathbf{t}\right)\mathrm{P}\left(t_{new}=k\right)}{\sum_{j} \mathrm{P}\left(x_{new}|t_{new}=j,\mathbf{X},\mathbf{t}\right)\mathrm{P}\left(t_{new}=j\right)}
    \label{eq:bayes}
\end{equation}

What the naive bayesian classifier asssumes that the likelihoods of the input features follow gaussian distributions where the mean and variance are found using maximum likelihood estimation. Predictions are then made by yielding the label with the distribution from which the new data object is most likely to have been sampled from.

\subsection{Quadratic Discriminant Analysis}
Discriminant analysis classifiers are classifiers that are able to set polynomial thresholds in the input feature space. Linear discriminant analysis classifers are able to set multiple linear thresholds, and as the name implies quadratic discriminant analysis classifiers are able to set quadratic boundaries \cite{scikit_learn}.

\subsection{Decision Tree Classifiers} \label{meth:dt}

Decision tree classifiers are classifiers that create hierarchies of rules that are used to make predictions of the target variable based on the values of the input variables \cite{python_machine_learning_2nd}. The advantages of decision tree classifiers is that they are highly interpretable because of their rule based structure, and do not require as much data as many other classifiers. The main disadvantage of decision trees are that they are prown to overfitting unless restrictions are set as to how many branches can be grown, and what the maximum depth of the tree can be. \cite{python_machine_learning_2nd}. For the implementation of the single decision tree classifier used in this work the ''Gini impurity criterion'' is used to choose which of the existing nodes would yield the split of highest quality, it uses the ''best'' strategy to choose splits at a node and is allowed a max depth of five nodes. \bigskip

''Extra Trees'', and ''Random Forest'' are two ensamble methods that are based on initiating many decision tree classifiers. The Random Forest method makes a prediction by averaging predictions of many different Decision Tree classifiers that are trained on separate random partitions of the dataset \cite{gen_rand_for}. When choosing which node to split, a randomized subset of the input features are used to make the choice. These two additions of random behaviour are meant to decouple the prediction error of individual trees, such that an averaged prediction of all the trees will have higher accuracy than any single tree, and will reduce the probability of overfitting \cite{scikit_learn}. The implementation of the Random Forest classifer used in this work instantiates ten different decision trees that use the ''Gini impurity criterion'' to measure the quality of a split, the trees are allowed a maximum depth of five nodes and are only allowed to consider a single feature when finding the best split. \bigskip

The Extra Trees classifier, also known as ''extremely randomized trees'' works similarily to Random Forest, but introduces one more source of ''randomness'' to the mix \cite{scikit_learn}. When training a Random Forest classifier one attempts to estimate the threshold of a node-split such that the discrimination between data objects is maximized based on their target variable value, for an Extra Trees classifier multiple thresholds are picked at random, and the threshold than maximizes discrimination of data objects based on their target variable value is chosen \cite{scikit_learn}. The implementation of the Extra Trees classifier used in this work uses 100 different decision trees, each using the ''Gini impurity criterion'' to measure the quality of a split and there is set no limitation to how deep the trees can be, or the number of features that can be considered during a node-split.

\subsection{Ada Boost Classifier}
The Ada Boost classifer is an ensamble models like Random Forest, and Extra Trees. What makes Ada Boost different from Random Forest, and Extra Trees is that it is not neccesarily restricted to using Decision Tree classifiers as base classifiers, and it uses a voting system to make predictions and train the base classifiers. The training process works by assigning weights to each of the training objects. In the first round of training, each object is assigned the same weight $1/N$, where $N$ is the number of training objects, in the preceding rounds the weights are updated by assigning smaller weights to the objects that the model is able to predict correctly and bigger weights to the objects predicted wrong, the training algorithm is repeated until the maximum number of iterations is reached or the accuracy converges to a fixed value \cite{scikit_learn}. For the implementation of the Ada Boost classifer used in this work use a Decision Tree classifier with the same parameters as the classifier in section \ref{meth:dt} is used, only with a maximum depth of one.

\section{Presentation of Results}

In chapter \ref{chap:results} the results of the different models will be presented in the form of three case studies. Each case study will focus on a single target variable, and aims to find which model group performs best at predicting the target variable in question. Recall that the three target variables that will be considered in this thesis are: Heart failure, patient diagnosis, and the indication of individual left ventricle segments. As mentioned earlier in the chapter, four models will be tested. The case studies will first deal with each model individually, where variants of the models with different hypermarameters will be tested on the different datasets. Then, the best performing model variation within the four main models will be used to for comparison. The supervised models will be assessed with the metrics: accuracy, sensitivity, specificity and \acrshort{dor}. The clustering methods evaluated at two cluster centers will be assessed with the same methods as the supervised models. The clustering methods evaluated at two to nine cluster centers will also be assessed with ARI to determine whether the models evaluated at a higher number of cluster centers could fit the data better. \bigskip

The results of each model group in every case study will be presented in the form of a distribution plot of the \acrshort{dor}, a scatter plot of sensitivity versus specificity, a table showing the five model variations that attain the highest \acrshort{dor}, and if the model group is a clustering model a table of the the five model variations that attain the highest \acrshort{ari} will also be presented. Recall the definition of the \acrshort{dor} from equation \eqref{eq:dor}, if a \acrshort{dor} is between 0 and 1 it indicates that the product of \acrshort{fp}, and \acrshort{fn} is greater than the product of \acrshort{tp}, and \acrshort{tn} meaning that the performance of the classifier model is bad. If the \acrshort{dor} attained by a model is greater than 1, that at least means that it attained more correct predictions than wrong predictions. For a balanced dataset random guessing should attain accuracy, sensitivity and specificity scores of approximately 50$\%$, hence scores below 50$\%$ can be considered as bad. For unbalanced datasets such as the patient-diagnosis dataset where there are 170 positives and 30 negatives it becomes a bit more complicated. Only guessing 1 will yield an accuracy of 85$\%$, a sensitivity of 1 and a specificity of 0. So the most valued attribute among the models will be a balanced trade-off between sensitivity, and specificity. \bigskip

The \acrshort{ari} ranges from $-1$ to $1$, where a score of 1 indicaties that the label distribution of two groupings are perfectly matched, a score close to 0 indicates that there is very little overlap between label distribution of the two groupings and a score close to $-1$ indicates that the overlap of the labels of two groupings is worse than what would be expected by two sets of random label distributions. The main strength of the \acrshort{ari} is also the reason why it is used by many authors to evaluate clustering models CITATION, it allows for the comparison of a set of cluster assignments where the number of clusters is greater than the number of labels.
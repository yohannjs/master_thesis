\chapter{Discussion} \label{chap:discussion}

In the results chapter, the performance results were presented in the order of the different target variables that were explored. In the discussion chapter, a different approach is taken, and each model will be discussed individually based on their performance in the case studies.

\section{Time-series Clustering} \label{sec:disc_tsc}

% \textbf{Give a brief summary of how the TSC model was implemented.}
Before dissimilarity was measured between strain curves, curves were preprocessed in one of four ways. 
Curves were either: not preprocessed, scaled between zero and one, normalized between zero and one, or z-score normalized. The \acrshort{tsc} model was implemented by using \acrshort{dtw} distance between strain curves as a dissimilarity measure to achieve a shape-based \acrshort{tsc} model. All the dissimilarity measures between a specific strain curve of one patient to the same strain curve of every other patient were combined into a dissimilarity matrix. If the dataset represented patients with more than one strain curve, the dissimilarity matrices of each individual strain curves were added together, such that there was a single dissimilarity matrix that represented the dissimilarity between the patients. The dissimilarity matrix was then passed to the hierarchical agglomerative clustering algorithm, which started out with each patient as an individual cluster and merged clusters based on specific linkage criteria. Seven linkage criteria were tested: single, complete, average, ward, centroid, median, and weighted. The clustering model was evaluated at all cluster centers between two and nine. The \acrshort{ari} was estimated for all the cluster assignments generated and the different target variables. For the cluster assignments yielded by a clustering model evaluated at two cluster centers, the accuracy, sensitivity, specificity, and \acrshort{dor} were also calculated. \bigskip

% \textbf{Mention briefly which placements TSC has in the different case studies.}
The \acrshort{tsc} models did not perform best in any of the case studies, but variations of the \acrshort{tsc} models generally yielded results with high performance in terms of accuracy, sensitivity, and specificity. In the heart failure case study the best variation of the \acrshort{tsc} model achieved the highest sensitivity and \acrshort{dor}, but it was outperformed by the best variation of the \acrshort{pvc} model overall. In the patient diagnosis case study the best variation of the \acrshort{tsc} model outperformed the best variation of the \acrshort{pvc} model, but they were both outperformed by the best \acrshort{pvsc} model. In the segment indication case study the best variation of the \acrshort{tsc} model attains the highest accuracy, specificity and \acrshort{dor}, but is outperformed by the \acrshort{ann} because it attains a higher sensitivity score, and thereby attains a more balanced accuracy in the positives and negatives. \bigskip
% \textbf{Discuss why methods using data from single view performed better.}
As discussed in section \ref{sec:curse_dimensionality} a challenge for all statistical models is the ''curse of dimensionality''. Briefly described, inmachine learning and data mining the curse of dimensionality refers to the issue of attaining a good balance between the number of dimensions that an object is represented in, and the number of objects used to train and/or evaluate the model. In the heart failure and patient diagnoses case studies the \acrshort{tsc} models that perform best in terms of \acrshort{dor}, and \acrshort{ari} are the models that use datasets where there are objects are represented by fewer dimensions. A reason for this could be that for 199 patients, the heart failure diagnoses, and patient diagnoses are most seperable for the \acrshort{tsc} models when only one strain curve is used. The curve that then gives the easiest separation of patients is then the \acrshort{2ch} \acrshort{gls} curve. In the heart failure study the \acrshort{tsc} models that attain the five best performing models in terms of \acrshort{dor} and \acrshort{ari} only use the \acrshort{gls} curve from the \acrshort{2ch} view, meaning that these methods only use one of 21 possible curves. This can be confirmed from table \ref{tab:tsc_hf_dor_sens_spec_dist} and \ref{tab:tsc_hf_ari}. In the patient diagnoses study one can see from table \ref{tab:tsc_ind_dor_sens_spec_dist} that the five methods that attain the highest \acrshort{dor} also only use the \acrshort{gls} curve from the \acrshort{2ch} view. These two observations support the claim that at a dataset size of 199 objects using fewer strain curves makes it easier for \acrshort{tsc} models to separate heart failure diagnoses, and patient diagnoses. An observation that does not directly support this claim is that in the patient diagnosis case study, the \acrshort{tsc} models that attain the four highest \acrshort{ari} use a combination of \acrshort{gls} and \acrshort{rls} curves in the \acrshort{4ch} view, or use the \acrshort{gls} curves from all views. However, these methods also only use three and seven of 21 curves in total, so this observation does not negate the claim entirely. \bigskip
% \textbf{Discuss why methods using data using ''regular'' or ''scaled'' performed better than methods using normalization or z-normalization.}
In all case studies it was found that \acrshort{tsc} models that performed best in terms of \acrshort{dor}, and \acrshort{ari} used no preprocessing. In some cases models using scaling as a form of preprocessing yielded the same cluster assignments, which could indicate that scaling the curves before measuring dissimilarity does not make much of a difference. Since \acrshort{tsc} models using normalization or z-score normalization as a form of preprocessing were not among the top five methods in terms of \acrshort{dor}, or \acrshort{ari} in any of the case studies the argument could be made that these form of preprocessing are not suited when using \acrshort{dtw} as a dissimilarity on left ventrice strain curves. \bigskip
% \textbf{Talk about the performance of different linkages.}
Of the seven linkages tested, it was the centroid, weighted and ward linkages that went into the \acrshort{tsc} models that performed best at predicting heart failure, patient diagnosis and segment indication respectively, in the different case studies. However, the single, complete and average linkages also went into the methods that appeared in the top five candidates in terms of \acrshort{dor}, or \acrshort{ari}. So it is not possible to say certainly that all linkages other than centroid, weighted and ward linkages are not suited for clustering left ventricle strain curves, but one can say with some degree of certainty that the median linkage does not go into any of the \acrshort{tsc} models that perform well in any of the three case studies. \bigskip
% \textbf{Talk about run-time challenges.}
When calculating the dissimilarity matrix of a set of 199 curves, it took approximately 0.3 seconds using the C-optimized functions of the dtaidistance library. The time it took to compute the clustering varied between 0.15 and 0.45 seconds depending on what linkage was used. The single linkage criteria was found to be the fastest, and the complete linkage was found to be the slowest. That the single linkage was the fastest could is to be expected, as it fairly easy to compute. However, it was unexpected that the complete linkage was the one that took the longest time to compute as one would expect the more complex linkages such as the ward linkage to take the longest time to compute. When the size of the dataset was increased to approximately 3600 curves it took 162 seconds to compute the dissimilarity matrix. This increase in run time is in agreement with the time complexity of the \acrshort{dtw} algorithm described in section \ref{sec:theory_diss}. In addition, the time it took to compute the clustering after the dissimilarity matrix was computed also increased to vary between 3 seconds for the single linkage, and 871 seconds for the ward linkage. So for a bigger dataset the run time of the different linkages were more as expected. Although these run-times are attained with a with a regular desktop Lenove G510 laptop, it illustrates possible challenge of how run-time of the calculations of the dissimilarity matrix, and clustering increase quadratically with the size of the dataset. \bigskip
% \textbf{Improvements that could be done on the existing model}
It was often found that the \acrshort{pvc} models that used \acrshort{ef} in addition to peak systolic strain values performed better than the \acrshort{pvc} models that only used strain values. It would be interesting to see whether incorporating \acrshort{ef} in the \acrshort{tsc} model would improve its performance as well. Since the hierarchical agglomerative clustering algorithm is uses dissimilarity matrix to cluster objects, it should be fairly straight-forward to calculate the dissimilarity matrix between a patients \acrshort{ef} values, and add that to the dissimilarity matrices of the indivial curves. One could also consider the approach taken by \cite{myocardial_motion_pattern}, where they split the strain curves into five smaller curves based on the different periods of heart cycle, and pass them to the model separately. Although the authors achieved good results with this, they also say that annotating points of every strain curve as systolic or diastolic is very time consuming.

\section{Peak-value Clustering}
% \textbf{Give a brief summary of how the \acrshort{pvc} model was implemented.}
The \acrshort{pvc} model was implemented in a similar fashion as the \acrshort{tsc} model.
The datapoints used to represent patients were passed to an implementation of hierarchical agglomerative clustering in scikit-learn. The dissimilarity between patients was measured as the Euclidean distance between the dimensions used to represent them. The scikit-learn implementation did not have all the same clustering linkages available as the scipy implementation used for \acrshort{tsc}, so only the following four linkages were tested: single, complete, average and ward. The evaluation procedure for the \acrshort{pvc} model was the same as the procedure used for \acrshort{tsc}. \bigskip
% \textbf{Mention briefly which placements \acrshort{pvc} has in the different case studies.}
The best variations of the \acrshort{pvc} model had a high performance in the heart failure, and in the patient diagnosis case studies. It was chosen as the best model in the heart failure case study, but was closely followed by the \acrshort{tsc}, and \acrshort{pvsc} models. In the patient diagnosis case study the best variation of the \acrshort{pvc} model attained the highest specificity and second-highest \acrshort{dor} of the three models compared. However, it was outperformed by both the \acrshort{tsc} and \acrshort{pvsc} models due to its low sensitivity. \bigskip
% \textbf{Note that the \acrshort{pvc} methods that use combination of peak systolic strain values and \acrshort{ef} performs significantly better, future work could be to combine \acrshort{ef} values with strain curves as well.}
In the heart failure case study, \acrshort{pvc} models that used datasets that were a combination of peak systolic strain values and \acrshort{ef} performed consistently better than the models than only used the strain values. This is expected in the heart failure case study, as \acrshort{ef} is a parameter that is well established in the current medical procedures used to diagnose patients with heart failure. In the heart failure case study it was found that an \acrshort{ef} threshold classifier outperformed all \acrshort{pvc} models, which for heart failure at least goes to show that for a point-value dataset of 199 objects strain values could be adding more noise than they are adding information, especially for the \acrshort{pvc} models. \bigskip
% \textbf{Talk about the performance of different linkages.}
In the heart failure case study, it was the complete linkage that was used in the model that was chosen as the best performer, but the ward and average linkages were also used in the models that attained the top five \acrshort{dor} and \acrshort{ari} scores. In the patient diagnosis case study, the complete linkage was also used in the model that was chosen as the best performer. Hence, for \acrshort{pvc} models using peak systolic strain values and \acrshort{ef} to identify heart failure among patients and patient diagnosis, the single linkage was not found to be suited. \bigskip
% \textbf{Talk about run-time challenges.}
Since a scikit learn implementation was used for the \acrshort{pvc} model, it was not possible to separate run-time of the dissimilarity calculation and the clustering itself. However, Euclidean distance is known to scale linearly with the number of dimensions per object and number of objects in the dataset. Since the underlying algorithm used by scikit learn is the same as the one used by scipy it is assumed that it would perform similarly to the \acrshort{tsc} model in terms of run time. \bigskip

\section{Neural Networks}
% \textbf{Give a brief summary of how the NN model was implemented.}
For the \acrshort{ann}, two types of preprocessing were tested in addition to the option of not preprocessing at all, upsampling the curves to the highest sample rate in the dataset and downsampling the curves to the lowest sample rate. The curves of the dataset were then passed as input to the \acrshort{ann} architecture detailed in section \ref{sec:ann_architecture} together with the relevant target variables. The \acrshort{ann} was trained for five epochs using \acrshort{sgd} with back-propagation. To validate the \acrshort{ann} models 10-fold cross-validation was used, at the end of each fold the \acrshort{tp}, \acrshort{tn}, \acrshort{fp}, and \acrshort{fn} of the model were noted. After the \acrshort{ann} had effectively attempted to predict every object of the dataset all the \acrshort{tp}, \acrshort{tn}, \acrshort{fp}, and \acrshort{fn} were summed and this grand total was used to estimate the models accuracy, sensitivity, specificity and \acrshort{dor}. \bigskip
% \textbf{Mention briefly which placements NN has in the different case studies.}
The \acrshort{ann} models performed worst of the four model groups in the heart failure case study, and the patient diagnosis case study. However, it attained the highest sensitivity in the segment indication case study. It was chosen as the best performing model because its sensitivity and specificity were more balanced than the \acrshort{tsc} model.
% \textbf{Talk about the performance of NN in different case studies.}
In the patient diagnosis case study close to all of the \acrshort{ann} models predicted all the patients to be unhealthy. It is evident that an \acrshort{ann} with the architecture used in this assignment was not suited to classify patient diagnoses with a skewed dataset of only 169 unhealthy patients and 30 healthy patients. It is the author's opinion that the reason that the \acrshort{ann} models performed so bad at predicting patient diagnosis is an aspect of ''the curse of dimensionality'', and that the network was not able to generalize the characteristics of healthy patients in the study, and therefore minimized loss function by predicting the most probable label (unhealthy). From table \ref{tab:dl_hf_raw_results}, one can see that the top nine variations of the \acrshort{ann} model that performed best in the heart failure case study with regard to \acrshort{dor}, were models that used only the \acrshort{gls} curve from a single view, which supports the claim that. Since the different \acrshort{ann} models differed in architecture depending on how many curves were used to represent one patient, they also varied in the number of trainable parameters they have. The \acrshort{ann} models that only take a single strain curve as input have 39457 trainable parameters, and the \acrshort{ann} models that take 21 curves as input have 80417 trainable parameters. Even though there is no exact ratio of how big a dataset should be with regard to how many trainable parameters a model has, between 40 and 80 thousand parameters for a dataset of size 199 is likely too many trainable parameters. On the other hand, the \acrshort{ann} model was chosen as the best performing model at predicting segment indication. However, in that case study the size of the dataset is significantly larger, and each object is represented by a single curve. Considering that the architecture of the \acrshort{ann} was given and not developed specifically for this classification problem, the performance that the model achieves is significant. \bigskip
% \textbf{Possible improvements}
It is the author's opinion that if more time is spent adapting the model to the dataset at hand, even better performances are within reach for the \acrshort{ann} models. Especially for the segment indication classification problem, since it is much bigger than the two other datasets. The first improvement that could be done the \acrshort{ann} models is to reduce its complexity by removing layers or removing filters and units in individual layers. There are alternatives to \acrshort{sgd} that could be tested, such as batch gradient descent and mini-batch gradient descent, which is a middle road between the two. There is also the \acrfull{gru} cells that are an alternative to the \acrshort{lstm} cells. Like \acrshort{lstm} cells, \acrshort{gru} cells are able capture time-dependent connections. \acrshort{gru} cells are simpler than \acrshort{lstm} cells in composition. One could also consider introducing layers that reduce complexity such as max-pooling layers, which for time series can be considered as a max-value filter where only the highest value in a segment of a curve is kept on. Dropout layers are also a technique that is used frequently when \acrshort{ann} architecture becomes deep and complex. Dropout layers introduce the probability that any particular perceptron in the layers preceding it can ''drop out'' meaning that they become inactive. In complex \acrshort{ann} architectures, it is often found that during training, the model becomes overly dependent on certain perceptrons, and specific paths through the network. This leads to the \acrshort{ann} not entirely utilizing all the perceptrons at its disposal, and the accuracy suffers. It is found that adding a dropout layer remedies this effect and can increase accuracy overall. \bigskip
% \textbf{Talk about run-time challenges.}
Training, and validating the \acrshort{ann} models were one of the more time-consuming computations required. The time it took to train the network depended on what dataset was used, which makes sense as increasing the number of curves the \acrshort{ann} can take as input also increases the number of trainable parameters that need to be trained for each step of the \acrshort{sgd} algorithm. When validating the \acrshort{ann} models, a single fold in the 10-fold cross-validation took approximately 100 seconds in the heart failure and patient diagnosis case studies. The time it took to execute one fold in the segment indication case study took approximately 640 seconds (11 min). However, these times do not reflect the times it will take to use the \acrshort{ann} to evaluate new cases after training, so the same challenge one has with clustering is not as pressing, should the goal be to deploy the \acrshort{ann} in a real-time clinical setting.

\section{Peak-value Supervised Classifiers} \label{sec:disc_pvsc}
% \textbf{Give a brief summary of how the \acrshort{pvsc} model was implemented.}
The different peak-value datasets are passed the different supervised classifiers in the model group. The different datasets are detailed in section \ref{sec:datasets}, and the different supervised classifiers tested are detailed in section \ref{sec:meth_pvsc}. Each combination of dataset and classifiers is validated by 10-fold cross-validation in the same manner as the \acrshort{ann}. 
% \textbf{Mention briefly which placements \acrshort{pvsc} has in the different case studies.}
In the heart failure case study, the best \acrshort{pvsc} model outperformed the best variations of the \acrshort{tsc}, and \acrshort{ann} models and had a performance that was on par with the \acrshort{pvc}, although the best \acrshort{pvc} model was ultimately deemed better in the end. In the patient diagnosis case study, the best \acrshort{pvsc} model attained the highest accuracy, sensitivity, and \acrshort{dor} of the four model groups, and it was deemed the best model group at predicting patient diagnosis. \bigskip
% \textbf{Mention that \acrshort{dor} distribution of \acrshort{pvsc} models were significantly higher centered than the other models groups \acrshort{dor} distribution.}
What should be addressed is the fact that the distribution of the \acrshort{dor} for the different \acrshort{pvsc} models differ from the \acrshort{dor} distributions of the other models in some key ways. In both the heart failure case study, and the patient diagnosis case study the distribution of \acrshort{dor} for variations of \acrshort{tsc}, \acrshort{pvc} and \acrshort{ann} models are highly concentrated around zero. For the \acrshort{pvsc} models the lowest \acrshort{dor} attained by a \acrshort{pvsc} model in the heart failure study is 1.94, and the lowest \acrshort{dor} attained by a \acrshort{pvsc} model in the patient diagnosis case study is 3.68. In the heart failure case study, it is especially evident that the \acrshort{dor} of the different \acrshort{pvsc} models is distributed differently than the \acrshort{dor} of the other models. It can be confirmed from figure \ref{fig:pvmlc_hf_dor_sens_spec_dis} that the distribution of \acrshort{dor} for the \acrshort{pvsc} is especially concentrated in the range between four to eight. The significance of this difference of \acrshort{dor} distribution is two-fold, the first thing to keep in mind is that not very much time was spent optimizing the hyperparameters of the \acrshort{pvsc} models as it falls outside the scope of this thesis, and that in contrast to the clustering models the outcome of the \acrshort{pvsc} model is probabilistic in the sense that it is highly dependent on the initial conditions of the model before it is trained. Since the \acrshort{dor} distribution of \acrshort{pvsc} models in the heart failure and patient diagnosis case studies are distributed higher in general than the \acrshort{tsc} and \acrshort{pvc} models, and that the \acrshort{pvsc} are configured with what can be considered as ''standard hyperparameters'' it is probable that spending time on optimizing the hyperparameters of the \acrshort{pvsc} models, and testing different initial conditions could improve the performance of all the \acrshort{pvsc} models. Additionally, the fact that the \acrshort{ef} threshold classifier outperformed the best \acrshort{pvc} model at identifying heart failure among patients is another indicator that there is untapped potential in the \acrshort{pvsc} models. \bigskip
% \textbf{Talk about run-time challenges.}
The time it took to train and validate the \acrshort{pvsc} models varied and was highly dependent on the dimensions of the dataset and which specific machine learning model was used. The shortest training time encountered was at 201 seconds, and the longest was at 365 seconds. These were the shortest training times encountered among the four model groups. Similar to the \acrshort{ann} model, the training times of the \acrshort{pvsc} models do not hinder their ability to make predictions in real-time and deploy them in a clinical setting.

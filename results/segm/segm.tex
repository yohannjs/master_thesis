\section{Case Study: Segment Indication}

\subsection{Time-series Clustering}

\begin{figure}[htb]
    \centering
    \input{results/segm/tsc_segm_ind_dor_sens_spec_dist.pgf}
    % \includegraphics[width=\textwidth]{results/tsc_segm_ind_dor_sens_spec_dist.png}
    \caption{Distribution of DOR, sensitivity and specificity for the different TSC methods when classifying left ventrice segment indication.}
    \label{fig:tsc_segm_ind_dor_sens_spec_dist}
\end{figure}

\begin{table*}[htb]
    \centering
    \ra{1.3}
    \begin{tabular}{lrrrr}
        \toprule
        Dataset-Method     &  Accuracy &  Sensitivity &  Specificity &  DOR \\
        \midrule
        regular/weighted/2 &      0.68 &         0.44 &         0.95 & 14.80 \\
        scaled/weighted/2  &      0.68 &         0.44 &         0.95 & 14.80 \\
        regular/ward/2     &      0.76 &         0.64 &         0.88 & 13.15 \\
        scaled/ward/2      &      0.76 &         0.64 &         0.88 & 13.15 \\
        regular/complete/2 &      0.74 &         0.60 &         0.89 & 12.89 \\
        \bottomrule
    \end{tabular}
    \caption{The accuracy, DOR, sensitivity and specicity scores of the five best performing two-cluster-center TSC methods in terms of DOR, at detecting segment indication.
             The \textbf{Dataset-Method} column indicates \textit{Type of preprocessing used}$/$\textit{Linkage criteria of method}$/$\textit{Number of cluster centers}.}
    \label{tab:tsc_segm_ind_dor_sens_spec_dist}
\end{table*}

\begin{comment}
[ ] \textbf{Comment on spread of DOR.}
[ ] \textbf{Comment on spread of sensitivity and specificity.}
[ ] \textbf{Comment on common traits in the high performing methods.} Here you can refer to raw performance results in appendix.
[ ] \textbf{Comment on common traits in the low performing methods.} Here you can refer to raw performance results in appendix.
[ ] \textbf{Select one - three methods that are good contendors for being the best method/model in the group and comment on their traits}
\textbf{IF NOT CLUSTERING METHOD}
[ ] \textbf{Make arguments for and against the top three methods in terms of accuracy, sensitivity, specificity, and DOR, and make an informed choice.}
\end{comment}

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lr}
        \toprule
        Dataset-Method     &  ARI \\
        \midrule
        scaled/centroid/5  & 0.26 \\
        regular/centroid/5 & 0.26 \\
        regular/ward/2     & 0.26 \\
        scaled/ward/2      & 0.26 \\
        scaled/centroid/6  & 0.25 \\
        \bottomrule
    \end{tabular}
    \caption{The five highest ARI scores attained when applying TSC for detecting segmend indication.
             The \textbf{Dataset-Method} column indicates \textit{Type of preprocessing used}$/$\textit{Linkage criteria of method}$/$\textit{Number of cluster centers}.}
    \label{tab:pvc_ind_ari}
\end{table*}

\begin{comment}
\textbf{ARI PARAGRAPH. ONLY FOR CLUSTERING METHODS}.
[ ] \textbf{Comment on the spread of ARI scores. Be specific since the distribution plots are ommitted}
[ ] \textbf{Comment on the general trends of high performing methods in terms of ARI - are they the same trends as scores performing high in terms of DOR?}
[ ] \textbf{Comment on whether the methods in the top 5 ARIs are the same methods with the highest DOR. If not, mention it.}
[ ] \textbf{If the top 1 or 2 ARIs are also top in DOR no further discussion is needed. You can then plot some of the cluster realizations to see what they look like.}
[ ] \textbf{If NOT, why do they differ? Is the method with the highest ARI evaluated at a higher cluster number that 2? Attempt to visualize it, if it is not too difficult.}
[ ] \textbf{Plot some visualizations of the clustering, and comment on them.}
[ ] \textbf{Make arguments for and against the top three methods in terms of accuracy, sensitivity, specificity, DOR, ARI and potentially the plots, and make an informed choice.}
\end{comment}

\newpage

\subsection{Deep Neural Network}

\begin{table*}[htb]
    \centering
    \ra{1.3}
    \begin{tabular}{lrrrr}
        \toprule
        Method      &  Accuracy &  Sensitivity &  Specificity &  DOR \\
        \midrule
        regular     &      0.74 &         0.80 &         0.68 & 8.65 \\
        downsampled &      0.74 &         0.74 &         0.75 & 8.38 \\
        upsampled   &      0.65 &         0.55 &         0.73 & 3.36 \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation metrics of the NN for classifying the binary indication of individual segments in the left ventricle.}
    \label{tab:NN_segm_ind_perf}
\end{table*}

\begin{comment}
[ ] \textbf{Comment on spread of DOR.}
[ ] \textbf{Comment on spread of sensitivity and specificity.}
[ ] \textbf{Comment on common traits in the high performing methods.} Here you can refer to raw performance results in appendix.
[ ] \textbf{Comment on common traits in the low performing methods.} Here you can refer to raw performance results in appendix.
[ ] \textbf{Select one - three methods that are good contendors for being the best method/model in the group and comment on their traits}
\textbf{IF NOT CLUSTERING METHOD}
[ ] \textbf{Make arguments for and against the top three methods in terms of accuracy, sensitivity, specificity, and DOR, and make an informed choice.}
\end{comment}

\subsection{Comparisons}

\begin{comment}
    [ ] 
\end{comment}


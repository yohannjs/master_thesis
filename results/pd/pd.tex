\section{Case Study: Patient Diagnosis}

\subsection{Time-series Clustering}

\begin{figure}[H]
    \centering
    % \includegraphics[width=\textwidth]{results/tsc_ind_dor_sens_spec_dist.png}
    \input{results/pd/tsc_ind_dor_sens_spec_dist.pgf}
    \caption{(a) Distribution plot of \acrshort{dor} of all \acrshort{tsc} models evaluated at two cluster centers when applied to classify patient diagnosis.
             (b) Scatter plot of the same models sensitivity, and specificity.}
    \label{fig:tsc_ind_dor_sens_spec_dist}
\end{figure}

From the distribution plot in figure \ref{fig:tsc_ind_dor_sens_spec_dist}a one can see that the majority of \acrshort{dor} are close to zero, but there are some models that acheive a \acrshort{dor} above 30. In the scatter plot in figure \ref{fig:tsc_ind_dor_sens_spec_dist}b one can see that the specificity of the models and range from $0.5$ to 1, and the sensitivity scores range from 0 to $0.93$. As with heart failure, the \acrshort{tsc} models that perform best in terms of \acrshort{dor} use data from a single view. The \acrshort{2ch} view, and \acrshort{gls} curves are the only view and curve that are used among the models that achieve the five highest \acrshort{dor}. From the table of all the model results in the appendix \ref{tab:tsc_ind_raw_results} one can see that the highest performing model in terms of \acrshort{dor} to use a dataset other than \acrshort{gls} curves alone is \textit{gls-rls/2CH/scaled/ward/2} and it achieves a \acrshort{dor} of $26.76$. One can also note that the highest performing model in terms of \acrshort{dor} that uses a view other than only \acrshort{2ch} is \textit{rls/all-views/normalized/weighted/2} which achieves a \acrshort{dor} of 25.56. The \acrshort{tsc} models that achieve the highest \acrshort{dor} scores all use no preprocessing, or scaling. From table \ref{tab:tsc_ind_dor_sens_spec_dist} one can see that the \acrshort{tsc} models that acheive the highest \acrshort{dor} scores are \textit{gls/2CH/regular/centroid/2}, and \textit{gls/2CH/scaled/centroid/2} which are the same two models that achieve the highest \acrshort{dor} in the heart failure case study.

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lrrrr}
        \toprule
        Dataset-model             &  Accuracy &  Sensitivity &  Specificity &   \acrshort{dor} \\
        \midrule
        gls/2CH/regular/centroid/2 &      0.74 &         0.71 &         0.93 & 33.47 \\
        gls/2CH/scaled/centroid/2  &      0.74 &         0.71 &         0.93 & 33.47 \\
        gls/2CH/scaled/average/2   &      0.73 &         0.69 &         0.93 & 30.71 \\
        gls/2CH/regular/average/2  &      0.73 &         0.69 &         0.93 & 30.71 \\
        gls/2CH/scaled/ward/2      &      0.71 &         0.67 &         0.93 & 27.49 \\
        \bottomrule
    \end{tabular}
    \caption{The accuracy, \acrshort{dor}, sensitivity and specicity scores of the five best performing two-cluster-center \acrshort{tsc} models in terms of \acrshort{dor}, at detecting patient diagnoses.
             The \textbf{Dataset-model} column indicates 
             \textit{Dataset used}$/$\textit{View used}$/$\textit{Type of preprocessing used}$/$\textit{Linkage criteria of model}$/$\textit{Number of cluster centers}.}
    \label{tab:tsc_ind_dor_sens_spec_dist}
\end{table*}

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lr}
        \toprule
        Dataset-model                   &  \acrshort{ari} \\
        \midrule
        gls-rls/4CH/regular/complete/2   & 0.36 \\
        gls/all-views/regular/weighted/2 & 0.34 \\
        gls/all-views/scaled/weighted/4  & 0.33 \\
        gls/all-views/scaled/weighted/3  & 0.33 \\
        gls/APLAX/regular/single/10      & 0.32 \\
        \bottomrule
    \end{tabular}
    \caption{The five highest \acrshort{ari} scores attained when applying \acrshort{tsc} for detecting patient diagnoses.
             The \textbf{Dataset-model} column indicates \textit{Dataset used}$/$\textit{View used}$/$\textit{Linkage criteria of model}$/$\textit{Number of cluster centers}.}
    \label{tab:tsc_ind_ari}
\end{table*}

The majority of the \acrshort{ari} scorer for all the \acrshort{tsc} models evaluated at two to nine cluster centers are centered around zero. As with the \acrshort{tsc} models attaining the highest \acrshort{dor} the models using no preprocessing or scaling acheive the highest \acrshort{ari} indices when used to identify patient diagnoses. In addition, the \acrshort{gls} curves are also most often part of the dataset for the \acrshort{tsc} models receiving the highest \acrshort{ari} when used to identify patient diagnoses. From table \ref{tab:tsc_ind_ari} one can see that the \acrshort{tsc} models receiving the five highest \acrshort{ari} scores, are not among the \acrshort{tsc} models that receive the highest \acrshort{dor} scores. The \acrshort{tsc} model \textit{gls-rls/4CH/regular/complete/2} attains the highest \acrshort{ari} score when applied to identify patient diagnoses, and achieves an accuracy of $0.84$, a sensitivity of $0.87$ a specificity of $0.69$ and a \acrshort{dor} $14.65$. The \acrshort{tsc} model \textit{gls/all-views/regular/weighted/2} achieves the second highest \acrshort{ari} when applied to identify patient diagnoses, and achieves an accuracy of $0.82$, a sensitivity of $0.81$ a specificity of $0.83$ and a \acrshort{dor} $21.06$. What should also be noted is that the \acrshort{tsc} models achieving the two highest \acrshort{ari} when applied to identify patient diagnoses are models evaluated at two cluster centers, which means that none of the \acrshort{tsc} models evaluated at cluster centers between three and nine can perform better than the ones evaluated at two cluster centers. It may seem strange that the ordered lists of \acrshort{dor}, and \acrshort{ari} are so different. The reason for this is not because \acrshort{dor} inherently values sensitivity higher than specificity, but stems from how the \acrshort{dor} is defined. Recall that $\mathrm{DOR = ( TP \times TN )/ (FP \times FN)}$, since the patient diagnoses dataset is skewed in favour of positives \acrshort{tp} has the potential of being as high as 170 while \acrshort{tn} can be as high as 30. Therefore the \acrshort{dor} will be higher for models with a high sensitivity than for models with an equally high sensitivity. In figure \ref{fig:five_members_gls_rls_4CH_regular_complete_two} curves of five random cluster members assigned by the \textit{gls/all-views/regular/weighted/2} model are plotted. As with the observations made with regard to figure \ref{fig:tsc_hf_best_meth_5_samples} it is not possible to make any conclusive statements as to what the similarities are based on such a small sample size. However, based on the small sample size in \ref{fig:five_members_gls_rls_4CH_regular_complete_two} it seems as though the curves in cluster 2 (column (b)) are smoother in shape, than the curves in cluster 1 (column (a)). The \acrshort{tsc} model that is chosen as the best model for identifying patient diagnoses is \textit{gls/all-views/regular/weighted/2}, because it achieves the second highest \acrshort{ari}, and because it's sensitivity and specificity are more balanced than the model attaining the highest \acrshort{ari} and the models that achieve higher \acrshort{dor}.

\clearpage

\begin{figure}[ht]
    \centering
    \input{results/pd/five_members_gls_all_views_regular_weighted_two.pgf}
    \caption{Here the curves of five random cluster members assigned by the \textit{gls/all-views/regular/weighted/2} model are plotted.
             Each row represents one of the seven possible strain curves in the \acrshort{4ch} view. Coloumn (a) and (b) represent cluster 1 and 2 respectively.
             To make it easier to visually separate the curves, only five random members from cluster 1 and 2 are included in the figure.}
    \label{fig:five_members_gls_rls_4CH_regular_complete_two}
\end{figure}

\clearpage

\subsection{Peak-value Clustering}

\begin{figure}[H]
    \centering
    % \includegraphics[width=\textwidth]{results/pvc_ind_dor_sens_spec_dist.png}
    \input{results/pd/pvc_ind_dor_sens_spec_dist.pgf}
    \caption{(a) Distribution plot of \acrshort{dor} of all PVC models evaluated at two cluster centers when applied to classify patient diagnosis.
             (b) Scatter plot of the same models sensitivity, and specificity.}
    \label{fig:pvc_ind_dor_sens_spec_dist}
\end{figure}

From the distribution plot in figure \ref{fig:pvc_ind_dor_sens_spec_dist}a one can see that the majority of the PVC models get \acrshort{dor} close to zero, but there are a few models that attain \acrshort{dor} above 30, and close to 40. From the scatter plot in \ref{fig:pvc_ind_dor_sens_spec_dist}b one can see that almost all the sensitivity scores are above $0.5$, while the specificity scores are concentrated in the areas $0$ to $0.25$ and $0.95$. As with the heart failure case study the PVC models that perform high in terms of \acrshort{dor} use a dataset that is a combination of peak systolic strain values and \acrshort{ef}. From table \ref{tab:pvc_ind_dor_sens_spec_dist} one can see that \textit{gls-EF/ward/2} and \textit{rls-EF/complete/2} are the two top performers in terms of \acrshort{dor}. \textit{gls-EF/ward/2} achieves a slightly higher specificity score, where as \textit{rls-EF/complete/2} attains a slightly higher specificity score.

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lrrrr}
        \toprule
        Dataset-model    &  Accuracy &  Sensitivity &  Specificity &   \acrshort{dor} \\
        \midrule
        gls-EF/ward/2     &      0.76 &         0.72 &         0.94 & 39.33 \\
        rls-EF/complete/2 &      0.77 &         0.74 &         0.93 & 37.61 \\
        gls-rls-EF/ward/2 &      0.76 &         0.72 &         0.93 & 35.16 \\
        gls-EF/average/2  &      0.74 &         0.70 &         0.94 & 34.90 \\
        gls-EF/complete/2 &      0.68 &         0.63 &         0.94 & 25.75 \\
        \bottomrule
    \end{tabular}
    \caption{The accuracy, \acrshort{dor}, sensitivity and specicity scores of the five best performing two-cluster-center PVC models in terms of \acrshort{dor}, at detecting patient diagnoses.
             The \textbf{Dataset-model} column indicates \textit{Dataset used}$/$\textit{Linkage criteria of model}$/$\textit{Number of cluster centers}.}
    \label{tab:pvc_ind_dor_sens_spec_dist}
\end{table*}

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lr}
        \toprule
        Dataset-model     &  \acrshort{ari} \\
        \midrule
        gls/average/6      & 0.29 \\
        gls/average/7      & 0.29 \\
        gls-rls/complete/3 & 0.28 \\
        rls-EF/complete/2  & 0.26 \\
        gls-EF/ward/2      & 0.25 \\
        \bottomrule
    \end{tabular}
    \caption{The five highest \acrshort{ari} scores attained when applying PVC for detecting patient diagnoses.
             The \textbf{Dataset-model} column indicates \textit{Dataset used}$/$\textit{Linkage criteria of model}$/$\textit{Number of cluster centers}.}
    \label{tab:pvc_ind_ari}
\end{table*}

The majority of the \acrshort{ari} scores of PVC models applied to identify patient diagnoses are centered around zero, but as one can see from table \ref{tab:pvc_ind_ari} there are a few models that acieve an \acrshort{ari} above $0.2$ close to $0.3$. For a change, the PVC models that perform best in terms of \acrshort{ari}, are neither models evaluated at two cluster centers, or models that are applied on a combination of peak systolic strain values and \acrshort{ef}. In contrast to the heart failure case study, the PVC models that achieve the highest \acrshort{ari}, when applied to identify patient diagnoses, are not the same models that achieve the highest \acrshort{dor}. The two PVC models that achieve the highest \acrshort{ari} are the \textit{gls/average} model evaluated at 6 and 7 cluster centers respectively. To get a better idea of why \textit{gls/average/6} and \textit{gls/average/7} attain the \acrshort{ari} they do, scatter plots of these two models, and \textit{gls-EF/ward/2} have been given in figure \ref{fig:scatter_gls_ef_hf_cluster_assignments}. A scatter plot of the target variable patient diagnosis is also given for comparison. The dimensions used are peak systolic \acrshort{gls} in all three views as these are the dimensions that are common to all three models. From the scatter plot in plot \ref{fig:scatter_gls_pd} one can see that the healthy patients are in the minority, and are concentrated in the corner with low peak systolic \acrshort{gls} values in the \acrshort{4ch}, \acrshort{2ch} and \acrshort{aplax} views. There are also some healthy patients with low-medium peak systolic \acrshort{gls} values, and very few healthy patients with high peak systolic \acrshort{gls} values. From plot \ref{fig:scatter_gls_ef_ward2_ind} one can see that \textit{gls-EF/ward/2} is able to isolate the concentration of healthy patients with low peak systolic \acrshort{gls}, but at the cost of many \acrshort{fn}. In plot \ref{fig:scatter_gls_average6} and \ref{fig:scatter_gls_average7} one can see that cluster 1 of model \textit{gls/average/6}, and cluster 2 of model \textit{gls/average/7} capture the healthy patients with low peak systolic \acrshort{gls}, but are unable of capturing the healthy patients with medium to high values. If one combines clusters 1 and 5 of \textit{gls/average/6}, and lets them represent healthy patients, and let the remaining clusters represent unhealthy patients the model attains an accuracy of $0.74$, a sensitivity of $0.70$, a specificity of $0.94$ and a \acrshort{dor} of $34.90$. If one combines clusters 2 and 5 of \textit{gls/average/7}, and lets them represent healthy patients, and let the remaining clusters represent unhealthy patients this model attains an accuracy of $0.74$, a sensitivity of $0.70$, a specificity of $0.94$ and a \acrshort{dor} of $35.94$. While the performance of the revised \textit{gls/average/6} and \textit{gls/average/6} models are good, they are still not as good as the performance of the top three performers in terms of \acrshort{dor}, which attain higher accuracy, sensitivity and \acrshort{dor}. Therefore, \textit{rls-EF/complete/2} is chosen as the best of the PVC models at identifying patient diagnosis, as it achieves the second highest \acrshort{dor}, and a more balanced sensitivity/specificity than \textit{gls-EF/ward/2} that attains the highest \acrshort{dor} score.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{results/pd/scatter_gls_indication_bin.png}
        \caption{Patient Diagnosis. \textbf{H} stands for \textbf{Healthy}, and \textbf{U} stands for \textbf{Unhealthy}}
        \label{fig:scatter_gls_pd}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{results/pd/scatter_gls_EF_ward2.png}
        \caption{\textit{GLS-EF Ward/2} cluster assignments.}
        \label{fig:scatter_gls_ef_ward2_ind}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{results/pd/scatter_gls_average6.png}
        \caption{\textit{GLS Average/6} cluster assignments.}
        \label{fig:scatter_gls_average6}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{results/pd/scatter_gls_average7.png}
        \caption{\textit{GLS Average/7} cluster assignments.}
        \label{fig:scatter_gls_average7}
    \end{subfigure}
    \caption{Scatterplot of peak \acrshort{gls} values in each view. Colors in the of the different dots are given by heart failure diagnosis, and cluster assignments of 
             \textit{gls-EF/ward/2}, \textit{average/6} and \textit{average/7} models. Numbers are not included on the axes because the point of the figure is to illustrate the separability 
             of clusters, and patient diagnosis.}
             \label{fig:scatter_gls_ind_cluster_assignments}
\end{figure}

\newpage

\subsection{Deep Neural Network}

\begin{figure}[H]
    \centering
    \input{results/pd/dl_ind_dor_sens_spec_dist.pgf}
    \caption{(a) Distribution plot of \acrshort{dor} of all \acrshort{ann} models when trained to classify patient diagnosis.
             (b) Scatter plot of the same models sensitivity, and specificity.}
    \label{fig:dl_ind_dor_sens_spec_dist}
\end{figure}

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lrrrr}
        \toprule
        Dataset-model              &  Accuracy &  Sensitivity &  Specificity &  \acrshort{dor} \\
        \midrule
        all-strain/4CH/upsampled   &      0.83 &         0.99 &         0.00 & 0.00 \\
        all-strain/2CH/regular     &      0.85 &         1.00 &         0.00 &  NaN \\
        gls/2CH/regular            &      0.85 &         1.00 &         0.00 &  NaN \\
        rls/2CH/regular            &      0.85 &         1.00 &         0.00 &  NaN \\
        all-strain/2CH/downsampled &      0.85 &         1.00 &         0.00 &  NaN \\
        \bottomrule
    \end{tabular}
    \caption{The accuracy, \acrshort{dor}, sensitivity and specicity scores of the five best performing variations of the \acrshort{ann} in terms of \acrshort{dor}, when trained to predict patient diagnoses.
             The \textbf{Dataset-model} column indicates \textit{Dataset used}$/$\textit{View used}$/$\textit{Whether curve has been upsampled, downsampled or is regular}.}
    \label{tab:dl_hf_dor_sens_spec_dist}
\end{table*}

From the distribution plot in figure \ref{fig:dl_ind_dor_sens_spec_dist} one can see that the collective performance of the different variations of the \acrshort{ann} trained to predict patient diagnosis is terrible. The \acrshort{dor} of all the models are either zero because the number of \acrshort{tn} attained are zero, or not defined because the number of \acrshort{fn} are zero. The sensitivities are all 1, or close to 1, and the specificities are all 0. It is evident that the \acrshort{ann} are not able to generalize the traits of the healthy patients from such a small dataset. The \acrshort{ann} models are will therefore not be discussed further with relation to prediction of patient diagnosis, and are not included in the comparison of the four model groups.

\newpage

\subsection{Peak-value Classifiers}

\begin{figure}[H]
    \centering
    % \includegraphics[width=\textwidth]{results/pvmlc_ind_dor_sens_spec_dist.png}
    \input{results/pd/pvmlc_ind_dor_sens_spec_dist.pgf}
    \caption{(a) Distribution plot of \acrshort{dor} of all PVSC models when trained to classify patient diagnosis.
             (b) Scatter plot of the same models sensitivity, and specificity.}
    \label{fig:pvmlc_ind_dor_sens_spec_dist}
\end{figure}

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lrrrr}
        \toprule
        Dataset-model          &  Accuracy &  Sensitivity &  Specificity &  \acrshort{dor} \\
        \midrule
        gls-rls-EF/Ada-Boost   &      0.95 &         0.97 &         0.79 & 138.42 \\
        gls-rls/KNN            &      0.93 &         0.95 &         0.82 &  84.53 \\
        rls-EF/Extra-Trees     &      0.93 &         0.96 &         0.75 &  76.50 \\
        gls-rls-EF/Extra-Trees &      0.93 &         0.97 &         0.71 &  75.00 \\
        gls-rls/Extra-Trees    &      0.93 &         0.97 &         0.71 &  75.00 \\
        \bottomrule
    \end{tabular}
    \caption{The accuracy, \acrshort{dor}, sensitivity and specicity scores of the five best performing PVSC models in terms of \acrshort{dor}, when trained to predict patient diagnosis.
             The \textbf{Dataset-model} column indicates \textit{Dataset used}$/$\textit{Specific machine learning model used}.}
    \label{tab:pvsc_hf_dor_sens_spec_dist}
\end{table*}

From the distribution plot in figure \ref{fig:pvmlc_ind_dor_sens_spec_dist} it might seem like the majority of the \acrshort{dor} scores are close to zero, but in that is due to the shear spread of \acrshort{dor} scores so it should be said explicitly that the lowest \acrshort{dor} score of a PVSC model is 3.68 and is attained by the \textit{gls/Gaussian-Process} model. The spread of \acrshort{dor} is so great that some models attain a \acrshort{dor} close to 100, and one model attains a \acrshort{dor} close to 150. From the scatter plot in figure \ref{fig:pvmlc_ind_dor_sens_spec_dist} one can see that the sensitivity ranges from $0.75$ to 1, and the specificity ranges from close to zero to approximately $0.95$. Among the top five PVSC models in terms of \acrshort{dor} are many different combinations of models, and datasets. Three of the five highest \acrshort{dor} scores are attained by Extra-Trees models, and the top two scores are attained by KNN and Ada Boost classifiers. \textit{gls-rls-EF/Ada-Boost} and \textit{gls-rls/KNN} are the two top PVSC performers with regard to \acrshort{dor}. \textit{gls-rls-EF/Ada-Boost} achieves the highest sensitivity of the two by two points, and \textit{gls-rls/KNN} achieves the highest specificity of the two by three points. Since sensitivity and specificity is weighted equally in this study \textit{gls-rls/KNN} is chosen as the best of the PVSC models trained to identify patient diagnoses.

\newpage

\subsection{Comparisons}

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lcccc}
        \toprule
        Dataset-model                                 &  Accuracy &  Sensitivity &  Specificity &  \acrshort{dor} \\
        \midrule
        \textbf{TSC}-gls/all-views/regular/weighted/2 &      0.82 &         0.81 &         0.83 & 21.06 \\
        \textbf{PVC}-rls-EF/complete/2                &      0.77 &         0.74 &         0.93 & 37.61 \\
        \textbf{PVSC}-gls-rls/KNN                     &      0.93 &         0.95 &         0.82 & 84.53 \\
        \midrule
        Dataset-model                                 &  \acrshort{tp} &  \acrshort{tn} &  \acrshort{fp} &  \acrshort{fn} \\
        \midrule
        \textbf{TSC}-gls/all-views/regular/weighted/2 & 136 &  24 &   5 &  31 \\
        \textbf{PVC}-rls-EF/complete/2                & 117 &  27 &   2 &  42 \\
        \textbf{PVSC}-gls-rls/KNN                     & 147 &  23 &  5  &   4 \\
        \bottomrule
    \end{tabular}
    \caption{A table comparing the best contenders within each model group for predicting patient diagnoses. 
             The top table compare the models by their accuracy, sensitivity, specificity and \acrshort{dor}, 
             and the bottom table shows the number of \acrshort{tp}, \acrshort{tn}, \acrshort{fp} and \acrshort{fn} that the different models attain on their respective datasets.}
    \label{tab:pd_compare}
\end{table*}

From the top table in \ref{tab:pd_compare} one can see that there is a significant difference in performance between the three models included for comparison. The \acrshort{tsc} model \textit{gls/all-views/regular/weighted/2} attains the second highest accuracy, sensitivity and specificity of the three models, but also attains the lowest \acrshort{dor}. The \acrshort{tsc} model can also be said to attain the most balanced scores in terms of sensitivity and specificity. The PVC model \textit{rls-EF/complete/2} attains the highest specificity, second highest \acrshort{dor}, but lowest sensitivity and accuracy of the three models. The PVSC model \textit{gls-rls/KNN} attains the highest accuracy, sensitivity and \acrshort{dor} of all the models, but it also achieves the lowest specificity of all the models. However, since the PVSC model is so close to the \acrshort{tsc} model in terms of specificity, and is so much better than the other two models in all other metrics, it is chosen as the best model of identifying patient diagnoses. This can be confirmed from the bottom table in \ref{tab:pd_compare}, where one can see that the \acrshort{pvsc} model only gets one \acrshort{tn} less than the \acrshort{tsc} model, but attains 11 more \acrshort{tp}. 



\newpage
\section{Case Study: Heart Failure}

\subsection{Time-series Clustering}

\begin{figure}[htb]
    \centering
    % \includegraphics[width=\textwidth]{results/tsc_hf_dor_sens_spec_dist.png}
    \input{results/hf/tsc_hf_dor_sens_spec_dist.pgf}
    \caption{(a) Distribution plot of DOR of all TSC methods evaluated at two cluster centers when applied to classify heart failure.
             (b) Scatter plot of the same methods sensitivity-, and specificity-scores.}
    \label{fig:tsc_hf_dor_sens_spec_dist}
\end{figure}

Figure \ref{fig:tsc_hf_dor_sens_spec_dist}a shows that the DOR is close to zero for many of the two-cluster-center methods, 
meaning that the size of $TP \times TN$ is small compared to $FP \times FN$. 
However, the best performing methods are able to acheive a DOR above ten, these methods are listed in table \ref{tab:tsc_hf_dor_sens_spec_dist}.
From the scatterplot in figure \ref{fig:tsc_hf_dor_sens_spec_dist}b one can see that the distribution of sensitivity, and specificity are quite widespread,
some scores are as high as 1, and some as low as zero. 
\bigskip

\begin{comment}
To include in this paragraph
[X] DOR is close to zero for the majority of the methods, which means nr TP * TN is small compared to FP * FN.
[X] From scatterplot one can see that select few methods acheive DOR above 10
[X] Comment on the spread of sensitivity and specificity.
[ ] Mention the two best performing methods in terms of DOR, and their sensitivity and specificity values.
[ ] 2CH is the only view used by the 5 methods with highest DORs. The next four methods use the APLAX view. Reference to the raw data results in the appendix.
[ ] The highest ranking method in terms of DOR that actually uses all views has a DOR of 6.75 and accuracy of 69 percent.
[ ] Meaning that too many views pose a problem with this size of dataset due to the curse of dimensionality.
[ ] The forms of preprocessing that is most often found among the methods that perform well are ''regular'' meaning ''no preprocessing'' and ''scaled''.
[ ] Meaning that normalizing and z-normalizing does not help with regard to classifying heart failure.
[ ] Transition into next chapter by mentioning that we will look a bit closer at what curves are in the clusters of the two different methods, and refer to the figure
    that will be plotting 5 random curves from each cluster for the two best performing methods. 
\end{comment}

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lrrrr}
        \toprule
        Dataset-Method             &  Accuracy &  Sensitivity &  Specificity & DOR \\
        \midrule
        gls/2CH/regular/centroid/2 &      0.76 &         0.87 &         0.64 & 11.72 \\
        gls/2CH/scaled/centroid/2  &      0.76 &         0.87 &         0.64 & 11.72 \\
        gls/2CH/regular/average/2  &      0.75 &         0.85 &         0.65 & 10.38 \\
        gls/2CH/scaled/average/2   &      0.75 &         0.85 &         0.65 & 10.38 \\
        gls-rls/2CH/scaled/ward/2  &      0.74 &         0.82 &         0.67 &  9.14 \\
        \bottomrule
    \end{tabular}
    \caption{The accuracy, DOR, sensitivity and specicity scores of the five best performing two-cluster-center TSC methods in terms of DOR, at detecting heart failure.
             The \textbf{Dataset-Method} column indicates \textit{Dataset used}$/$\textit{View used}$/$\textit{Type of preprocessing used}$/$\textit{Linkage criteria of method}$/$\textit{Number of cluster centers}.}
    \label{tab:tsc_hf_dor_sens_spec_dist}
\end{table*}

\begin{figure}[htb]
    \centering
    \input{results/hf/five_members_gls_2CH_regular_centroid_two.pgf}
    \caption{Each plot depicts the 2CH GLS curves for five random cluster members. (a) and (b) contain members from cluster 1 and 2 respectively.}
    \label{fig:tsc_hf_best_meth_5_samples}
\end{figure}

\begin{table*}[htb]
    \centering
    \ra{1.3}
    \begin{tabular}{lr}
        \toprule
        Dataset-Method             &  ARI \\
        \midrule
        gls/2CH/regular/centroid/2 & 0.25 \\
        gls/2CH/scaled/centroid/2  & 0.25 \\
        gls/2CH/scaled/centroid/3  & 0.24 \\
        gls/2CH/regular/centroid/3 & 0.24 \\
        gls/2CH/scaled/average/2   & 0.24 \\
        \bottomrule
    \end{tabular}
    \caption{The five highest ARI scores attained when applying TSC for detecting heart failure.
             The \textbf{Dataset-Method} column indicates \textit{Dataset used}$/$\textit{View used}$/$\textit{Linkage criteria of method}$/$\textit{Number of cluster centers}.}
    \label{tab:tsc_hf_ari}
\end{table*}

As mentioned in section REFERENCE DOR, sensitivity and specificity are only well defined for clustering methods evaluated at two cluster centers,
so to determine whether the same clustering methods evaluated at a different number of cluster centers the ARI is used.
From table \ref{tab:tsc_hf_ari} one can see that the two methods with the highest ARI ($0.25$) both are clustering methods evaluated at two cluster centers. 
Hence, no exploration will be done to see if TSC methods evaluated at a higher number of cluster centers can identify heart failure among patients.

\begin{comment}
To include in this paragraph
[ ] Explain that five random curves are included because any higher number of curves were found to be confusing, and hard to look at. 
[ ] State that it is hard to make any conclusive statements on what are the specific similarities between curves of the different cluster members.
[ ] However, from figure \ref{fig:tsc_hf_best_meth_5_samples} it seems like the curves of cluster 2 are smooth, while the curves of cluster 1 are more irregular in shape.
[ ] Which makes sense as this clustering algorithm uses a shape-based distance measure. 
[ ] These curves, and cluster assignments are taken from the \textit{gls/2CH/regular/centroid/2} method, but it turns out \textit{gls/2CH/regular/centroid/2} has the exact same 
    cluster assignments, which explains why they acheive the exact same scores in every evaluation metric.
[X] Mention why we use ARI (because DOR, sensitivity, specificity, etc. are not well defined for clustering methods evaluated at a different number of cluster centers than the number of labels).
[X] Mention that the methods with the highest ARI, are methods evaluated at two cluster centers, hence this paper has not explored whether clustering methods at a greater number of cluster centers
    could be used for identifying heart failure.
[ ] So, since \textit{gls/2CH/regular/centroid/2} is one of two methods to acheive the highest DOR ($11.72$), accuracy ($0.76$), and ARI ($0.25$) it is chosen as the best of the TSC methods
    at identifying heart failure among patients.
[ ] \textit{gls/2CH/regular/centroid/2} is chosen over \textit{gls/2CH/scaled/centroid/2} because it does not require preprocessing.
\end{comment}

\begin{comment}
\end{comment}

\newpage

\subsection{Peak-value Clustering}

\begin{figure}[htb]
    \centering
    % \includegraphics[width=\textwidth]{results/pvc_hf_dor_sens_spec_dist.png}
    \input{results/hf/pvc_hf_dor_sens_spec_dist.pgf}
    \caption{(a) Distribution plot of DOR of all PVC methods evaluated at two cluster centers when applied to classify heart failure.
             (b) Scatter plot of the same methods sensitivity-, and specificity-scores.}
    \label{fig:pvc_hf_dor_sens_spec_dist}
\end{figure}

From figure \ref{fig:pvc_hf_dor_sens_spec_dist}a one can see that the DOR scores are substantially higher for PVC methods evaluated at two cluster centers to predict heart failure, 
than they are for the corresponding TSC methods. 
The scatterplot in figure \ref{fig:pvc_hf_dor_sens_spec_dist}b also shows that their exist multiple methods with both sensitivity and specificity above $0.6$ for the same PVC methods.
The exact metrics for the top performing PVC methods at predicting heart failure are given in table \ref{tab:pvc_hf_dor_sens_spec_dist}.
Common to the three highest performing PVC methods is that they all use the dataset that is a combination of peak systolic GLS values and EF values.
The highest DOR recorded is acheived when using the Ward linkage criteria, but it is not given that this is the ''best'' method.
The \textit{gls-EF/complete/2} method acheives a specificity score that is nine points higher at the cost of the sensitivity being six points lower, 
and it also has the highest overall accuracy of all the PVC methods by a small margin of one point. 
To get a better idea of how the different cluster methods perform at identifying heart failure, a scatterplot of the clusters is depicted in figure 
\ref{fig:scatter_gls_ef_hf_cluster_assignments}. \bigskip

\begin{comment}
    [ ] 
\end{comment}

\begin{table*}[htb]
    \centering
    \ra{1.3}
    \begin{tabular}{lrrrr}
        \toprule
        Dataset-Method    &  Accuracy &  Sensitivity &  Specificity &   DOR \\
        \midrule
        gls-EF/ward/2     &      0.75 &         0.87 &         0.63 & 11.59 \\
        gls-EF/complete/2 &      0.76 &         0.81 &         0.72 & 10.85 \\
        gls-EF/average/2  &      0.75 &         0.85 &         0.65 & 10.58 \\
        rls-EF/complete/2 &      0.73 &         0.86 &         0.60 &  8.89 \\
        gls-rls-EF/ward/2 &      0.72 &         0.84 &         0.60 &  7.80 \\
        \bottomrule
    \end{tabular}
    \caption{The accuracy, DOR, sensitivity and specicity scores of the five best performing two-cluster-center PVC methods in terms of DOR, at detecting heart failure.
             The \textbf{Dataset-Method} column indicates \textit{Dataset used}$/$\textit{Linkage criteria of method}$/$\textit{Number of cluster centers}.}
    \label{tab:pvc_hf_dor_sens_spec_dist}
\end{table*}

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{results/hf/scatter_gls_EF_hf.png}
        \caption{Heart failure.}
        \label{fig:scatter_gls_ef_hf}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{results/hf/scatter_gls_EF_ward2.png}
        \caption{\textit{Ward/2} cluster assignments.}
        \label{fig:scatter_gls_ef_ward2}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{results/hf/scatter_gls_EF_complete2.png}
        \caption{\textit{Complete/2} cluster assignments.}
        \label{fig:scatter_gls_ef_complete2}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{results/hf/scatter_gls_EF_average2.png}
        \caption{\textit{Average/2} cluster assignments.}
        \label{fig:scatter_gls_ef_average2}
    \end{subfigure}
    \caption{Scatterplot of peak GLS values in each view. Colors in the of the different dots are given by heart failure diagnosis, and cluster assignments of 
             ward/2, complete/2 and average/2 methods. Numbers are not included on the axes because the point of the figure is to illustrate the separability 
             of clusters, and heart failure.}
             \label{fig:scatter_gls_ef_hf_cluster_assignments}
\end{figure}

\begin{table*}[htb]
    \centering
    \ra{1.3}
    \begin{tabular}{lr}
        \toprule
        Dataset-Method    &  ARI \\
        \midrule
        gls-EF/complete/2 & 0.27 \\
        gls-EF/ward/2     & 0.24 \\
        gls-EF/average/2  & 0.24 \\
        rls-EF/complete/2 & 0.21 \\
        gls-EF/complete/3 & 0.21 \\
        \bottomrule
    \end{tabular}
    \caption{The five highest ARI scores attained when applying PVC for detecting heart failure.
             The \textbf{Dataset-Method} column indicates \textit{Dataset used}$/$\textit{Linkage criteria of method}$/$\textit{Number of cluster centers}.}
    \label{tab:pvc_hf_ari}
\end{table*}

In figure \ref{fig:scatter_gls_ef_hf_cluster_assignments} scatterplots patients are plotted with the dimensions: 4-chamber peak systolic GLS, 2-chamber peak systolic GLS and EF. 
The colors of the points correspond to wheather the patient has heart failure or not, and which cluster the points belong to.
The plots are actually a lower dimensional projection of the GLS-EF peak-value dataset. 
This particular projection was chosen as it was found to be the projection where heart failure patients were as separable as possible. 
From plots \ref{fig:scatter_gls_ef_hf_cluster_assignments}b-d one can see that the clusters are fairly separable, 
heart failure on the other hand is not as easy to separate in these dimensions as can be seen in plot \ref{fig:scatter_gls_ef_average2}. 
\textit{Ward/2} and \textit{Complete/2} can in some sense be considered as binary classifiers where values under a certain threshold are categorized as heart failure.
The \textit{ward/2} method has the highest threshold for what is considered heart failure, and \textit{complete/2} has the lowest, 
which explains their difference in sensitivity and specificity score. 
From figure \ref{fig:pvc_hf_ari} one can see that the many of the ARI of PVC methods for classifying heart failure are close to zero, but substantially more of the methods score above zero in ARI
than the TSC methods, as can be seen by a comparison of figure \ref{fig:tsc_hf_ari} and \ref{fig:pvc_hf_ari}. Table \ref{tab:pvc_hf_ari} shows that the three highest ARIs are attained by the same
three methods that acheived the highest DORs. This means that there are most likely no methods evaluated at a higher number of cluster centers that will outperform \textit{ward/2},
or \textit{complete/2} at classifying heart failure. In addition, the conclusion will be that \textit{complete/2} is the best performing PVC method when classifying heart failure, 
since it has the highest overall accuracy (76$\%$), highest ARI (0.27), and second highest DOR (10.85). 

\begin{comment}
    [ ] 
\end{comment}

\newpage 

\subsection{Deep Neural Network}

\begin{figure}[htb]
    \centering
    % \includegraphics[width=\textwidth]{results/dl_hf_dor_sens_spec_dist.png}
    \input{results/hf/dl_hf_dor_sens_spec_dist.pgf}
    \caption{(a) Distribution plot of DOR of all NN models evaluated at two cluster centers when trained to predict heart failure.
             (b) Scatter plot of the same models sensitivity-, and specificity-scores.}
    \label{fig:dl_hf_dor_sens_spec_dist}
\end{figure}

From the distribution plot in figure \ref{fig:dl_hf_dor_sens_spec_dist}a one can see that the most frequent DOR by the NN models is zero when training them to predict heart failure.
In the scatterplot in figure \ref{fig:dl_hf_dor_sens_spec_dist}b one can see that sensitivity scores vary between $0.15$ and $0.65$, and the specificity scores vary between $0$ and $0.68$.
The highest DOR of $1.36$ is attained by using only the GLS curve from the 4-chamber view as input, as can be seen from table \ref{tab:dl_hf_dor_sens_spec_dist}.
In fact the five highest DORs attained by NN models trained to classify heart failure are acheived using only curves from a single view as input.
There does not seem to be a particular view that is favored, as 4-chamber view, 2-chamber view and apical-view are are all found in the NN variations in table \ref{tab:dl_hf_dor_sens_spec_dist}.
The overall accuracy of the model variations are also fairly low, $0.54$ being the highest accuracy acheived.
Since the heart failure dataset is fairly evenly distribution (recall figure \ref{fig:hf_ind_dist}) an accuracy of $0.54$ is not much better than what could be acheived 
by randomly guessing the label. 
However, \textit{gls/4CH/upsampled} will be considered the best model variation of the NN at predicting heart failure since it acheives the highest accuracy (0.54) and DOR (1.36). \bigskip

\begin{comment}
    [ ] 
\end{comment}

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lrrrr}
        \toprule
        Dataset-Model         &  Accuracy &  Sensitivity &  Specificity &  DOR \\
        \midrule
        gls/4CH/upsampled     &      0.54 &         0.46 &         0.61 & 1.36 \\
        rls/APLAX/regular     &      0.53 &         0.48 &         0.58 & 1.30 \\
        rls/4CH/regular       &      0.52 &         0.36 &         0.68 & 1.20 \\
        gls/APLAX/downsampled &      0.52 &         0.63 &         0.40 & 1.15 \\
        gls/2CH/downsampled   &      0.51 &         0.61 &         0.40 & 1.03 \\
        \bottomrule
    \end{tabular}
    \caption{The accuracy, DOR, sensitivity and specicity scores of the five best performing variations of the NN in terms of DOR, at detecting heart failure.
             The \textbf{Dataset-Model} column indicates \textit{Dataset used}$/$\textit{View used}$/$\textit{Whether curve has been upsampled, downsampled or is regular}.}
    \label{tab:dl_hf_dor_sens_spec_dist}
\end{table*}

\newpage

\subsection{Peak-value Supervised Classifiers}

\begin{figure}[htb]
    \centering
    % \includegraphics[width=\textwidth]{results/pvmlc_hf_dor_sens_spec_dist.png}
    \input{results/hf/pvmlc_hf_dor_sens_spec_dist.pgf}
    \caption{Distribution of DOR, sensitivity and specificity for the different peak-value classifiers trained to predict heart failure.}
    \label{fig:pvmlc_hf_dor_sens_spec_dis}
\end{figure}

From the distribution plot depicted in figure \ref{fig:pvmlc_hf_dor_sens_spec_dis}a one can see that the PVSC models overall acheive relatively high DORs, with a range of approximately two to nine.
The scatterplot in figure \ref{fig:pvmlc_hf_dor_sens_spec_dis}b shows that the models are quite concentrated in terms of sensitivity and specificity scores. 
The majority of the models acheive sensitivity, and specificity scores in the ranges $0.6$ to $0.75$, with some outliers acheiving specificity below $0.5$ and sensitivity above $0.75$.
What is even more concentrated are the overall accuracy scores of the models. 
As can be seen in table \ref{tab:pvmlc_hf_dor_sens_spec_dis}, the accuracy of top five PVSC models are all $0.75$
The table also shows that the highest DOR of $9.4$ is acheived by model \textit{gls-EF/Gaissian-Process}. 
In general, all the best performing PVSC models use a combination of EF and peak systolic GLS or RLS values, 
and noe specific ML model seems to outperform the others on all the datasets in term of classification.
Although the DOR, sensitivity and specificity scores are very similar for the five best performing models \textit{gls-EF/Gaissian-Process} 
is chosen as the PVSC model that performs best at predicting heart failure.

\begin{comment}
    [ ] 
\end{comment}

\begin{table*}
    \centering
    \ra{1.3}
    \begin{tabular}{lrrrr}
        \toprule
        Dataset-Model           &  Accuracy &  Sensitivity &  Specificity &  DOR \\
        \midrule
        gls-EF/Gaussian-Process &      0.75 &         0.78 &         0.73 & 9.40 \\
        rls-EF/MLP              &      0.75 &         0.76 &         0.74 & 9.37 \\
        rls-EF/Linear-SVM       &      0.75 &         0.75 &         0.74 & 8.86 \\
        gls-EF/Ada-Boost        &      0.75 &         0.77 &         0.73 & 8.85 \\
        gls-EF/Naive-Bayes      &      0.75 &         0.76 &         0.74 & 8.79 \\
        \bottomrule
    \end{tabular}
    \caption{The accuracy, DOR, sensitivity and specicity scores of the five best performing PVSC in terms of DOR, at detecting heart failure.
             The \textbf{Dataset-Model} column indicates \textit{Dataset used}$/$\textit{The specific ML model used}.}
    \label{tab:pvmlc_hf_dor_sens_spec_dis}
\end{table*}

\subsection{Comparisons}

\begin{comment}
    [ ] 
\end{comment}

\newpage


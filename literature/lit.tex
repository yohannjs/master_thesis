\chapter{Review of The Literature} \label{chap:lit}

Two papers are considered particularly relevant to this work, and will be discussed in this paper. Both papers extract characteristic curves from the segments of the left ventricle using echocardiography, and both of the papers apply machine learning models to attempt to separate \acrshort{hfpef} patients from the rest. However, the different approaches used to fulfill this objective are quite different. 

\textcite{hf_diagnosis_ml} attempt to diagnose \acrshort{hfpef} using a combination of a statistical unsupervised method, and a supervised classifier. Their dataset consists of the velocity, strain and strain-rate curves of the 18 regional segments of the left ventrice in 100 subjects. The effectiveness of the velocity, strain and strain-rate curves were evaluated separately. The patients were stress tested, meaning that the curves were extracted at rest, and after having been exposed to a period of physical activity. This resulted in 36 curves of each curve type (18 at rest and 18 during exercise). The patients were split into four groups: \acrshort{hfpef} patients, hypertensive patients, healthy control patients and breathless control patients. They also performed another partitioning of the patients where they combined the hypertensive patients and the healthy patients, and combined the breathless patients and the \acrshort{hfpef} patients. The machine learning algorithm they developed was trained at predicting two different target variables, group affiliation with four classes, and group affiliation with two classes. The machine learning algorithm was composed of a unsupervised method called principle component analysis and variations of a supervised classifier called \acrfull{knn}. Principle component analysis is a method commonly used for dimensionality reduction. It projects the input variables onto a new set of dimensions where the the variance in the data is maximized. These new dimensions are linear combinations of the input dimensions and are called principle components. In terms of linear algebra, the principle components are the eigenvectors of the covariance matrix of the input variables. Principle component analysis was used in this work to reduce  dimensionality, such that there were fewer dimensions for the \acrshort{knn} classifiers to consider. The best performing model was the model that used strain curves as input. In the four-class classification problem the strain-curve-input model attained an overall accuracy of 0.57, but attained an accuracy of 0.81 within the class of heart failure patients with preserved \acrshort{ef}. In the two-class classification problem the same model attained an accuracy of 0.85, a sensitivity of 0.86 and a specificity of 0.82. The performance in the two-class classification problem is good and it is promising to see that strain curve input yielded the highest scores. \bigskip

\textcite{myocardial_motion_pattern} also deal with \acrshort{hfpef} patients. They do not attempt to make direct predictions about diagnosis, but use an unsupervised learning approach to study the patterns of the velocity curves of the myocard segments of patients with \acrshort{hfpef}. They reduce the dimensionality in an attempt to attain a representation that is easier to interpret. The machine learning approach they use is based on merging the features yielded by multiple non-linear operators, or kernels, and the method is called multiple kernal learning. This yields what the authors refer to as an ''output space'', which is a representation of the input data with reduced dimensionality. After the output space is constructed, they reconstruct the velocity curves using multiscale regression to see if this has an effect on the variability of the different velocity-curve features. Their model was a applied on a group of 55 patients consisting of 19 \acrshort{hfpef} patients, 22 healthy patients and 14 breathless patients. \textcite{myocardial_motion_pattern} also stress test all their patients. They extract four velocity curves from each patient and consider two types of analysis: One where the velocity curves are considered as a whole and another where the velocity curves are split into five, based on which period of the cardiac cycle they are in, yielding a total of 20 curves. Their analysis of the variability of the velocity curves showed promise in terms of being able to separate healthy patients from diseased. \textcite{myocardial_motion_pattern} also make some compelling arguments as to why unsupervised models could be preferred over supervised models. Unsupervised models are able to extract hidden structures in the data that are impossible to find with supervised models. Also, unsupervised models are not as hindered by objects that have been labelled wrong as they are not trained with labels. \bigskip

\textcite{hf_diagnosis_ml} and \textcite{myocardial_motion_pattern} make use of an unsupervised learning approach to reduce the dimensionality of the dataset. \textcite{hf_diagnosis_ml} use principal component analysis as a subroutine, which yield the principle components to a \acrshort{knn} classifier. \textcite{myocardial_motion_pattern} use multiple kernal learning to compress the input features into a representation that makes it eases clinical interpretation. Also, they reconstruct the original velocity curves using multiscale regression to analyse the varibility of the reconstructed time series. One of the big lessons from these two papers is the importance of dimensionality reduction when dealing with relatively small datasets of high dimensionality. Extra emphasis is given to the word "relatively small" because machine learning models in other fields such as computer vision often have access to thousands, if not millions, of data objects (recall the ImageNet database mentioned in section \ref{sec:motivation}). In the papers of \textcite{hf_diagnosis_ml} and \textcite{myocardial_motion_pattern} dimensionality reduction is attained by combining the initial features using principle component analysis and multiple non-linear kernels. In this work the approach will be slightly different, no attempt will be done at combining the features into a smaller subset, but the different models that are applied will be tested on different subsets of the peak-value and time-series datasets. Hence, it is more of a feature selection than a feature extraction. The different subsets of the datasets will be detailed further in section \ref{chap:data}.